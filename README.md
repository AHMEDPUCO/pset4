#  Proyecto 04 ‚Äî Data Mining  
**Predicci√≥n de `total_amount` en viajes de taxi NYC TLC (2015‚Äì2025)**

---

## Flujo general

El flujo abarca las siguientes etapas:

**Ingesta masiva usando pySpark ‚Üí Limpieza e integraci√≥n  de datos en postgres ‚Üí Construcci√≥n OBT en postgres ‚Üí ML predictivo (from scratch y scikit learn)**


## Arquitectura general del proyecto

| Servicio Docker   | Descripci√≥n                                                                 |
|-------------------|-----------------------------------------------------------------------------|
| `spark-notebook` | Entorno Jupyter + Spark para ingesta, exploraci√≥n de datos y entrenamiento de modelos                |
|  `postgres`        | Almacenamiento estructurado de datos en esquemas `raw` y `analytics`        |
|  `obt-builder`     | Script CLI para construir la OBT (`analytics.obt_trips`) desde `raw.*`     |

---

##  Variables de entorno (`.env`)

```bash
PG_HOST=postgres
PG_PORT=5432
PG_DB=nyc_taxi
PG_USER=postgres
PG_PASSWORD=postgres
PG_SCHEMA_RAW=raw
PG_SCHEMA_ANALYTICS=analytics
```

Estas variables aseguran que todos los contenedores compartan la misma configuraci√≥n y base de datos, garantizando reproducibilidad total del flujo.

## Ingesta con Spark ‚Üí Postgres

El notebook 01_ingesta_parquet_raw.ipynb permite descargar, leer y cargar los archivos Parquet de cada servicio (yellow, green) hacia Postgres.

üîß Modo de operaci√≥n
```bash
# --- Configuraci√≥n ---
YEARS = list(range(2015, 2026))  # 2015-2025
MONTHS = list(range(1, 13))      # 1-12
SERVICES = ['yellow','green']   # Tipos de taxi: green,yellow

```

 Idempotencia: Verificamos si ya fue subido un a√±o para no volver a subirlo. El mode "overwrite" permite que se sobreescriban los datos si es necesario.

```bash
write_batch(df_final, table, mode="overwrite", writers=4, batchsize=5000)

def is_already_ingested(service, year, month, table):
    """
    Retorna True si ya hay datos para ese service, a√±o y mes.
    """
    try:
        df_check = spark.read \
            .format("jdbc") \
            .option("url", "admin@postgres:5432/nyc_taxi") \
            .option("dbtable", table) \
            .option("user", PG_USER) \
            .option("password", PG_PASSWORD) \
            .load() \
            .filter((col("service_type") == service) & (col("year") == year) & (col("month") == month))
        
        return df_check.count() > 0
    except Exception as e:
        print(f"    [WARNING] No se pudo verificar existencia: {e}")
        return False

```
 Ventajas de la ingesta

No genera duplicados.

Limpieza estructural desde el primer paso y eliminaci√≥n de valores fuera de rangos.

Descarga temporal y  borrado autom√°tico para optimizar espacio en disco.


##  Construcci√≥n de la One Big Table (build_obt.py)

El servicio **obt-builder** integra los datos desde `raw.*` hacia `analytics.obt_trips`, aplicando limpieza, joins y normalizaci√≥n de tipos.

---

###  Ejecuci√≥n

```bash
docker compose run --rm obt-builder --mode by-partition --year-start 2015 --year-end 2015 --months 1 --services green --run-id docker_test --overwrite
```
 Principales caracter√≠sticas

Idempotencia: elimina la partici√≥n correspondiente (servicio, a√±o, mes) antes de insertar nuevos datos.

Se crean columnas opcionales din√°micas:

- airport_fee

- cbd_congestion_fee

- trip_type

Joins geogr√°ficos: se realiza uni√≥n con taxi_zone_lookup para enriquecer los datos con:

- borough (pickup y dropoff)

- zone (pickup y dropoff)

---


##  ML Notebook ‚Äî `ml_total_amount_regression.ipynb`

ML Notebook: Predicci√≥n de Monto Total del Viaje
================================================

Objetivo
--------

Predecir total\_amount al momento del pickup usando exclusivamente variables conocidas antes o durante el inicio del viaje, evitando data leakage.

Ingesta y Muestreo
------------------

**Fuente:** analytics.obt\_trips (Postgres) con filtros de calidad (sin nulos en pickup, distancias razonables, etc.).

**Muestreo determin√≠stico y equitativo por mes:**

*   Par√°metro rows\_per\_year ‚Üí se reparte en 12 meses (per\_month = rows\_per\_year // 12, con +1 en los primeros extra meses)
    
*   Selecci√≥n sin aleatoriedad v√≠a NTILE(...) por (year, month) y ROW\_NUMBER(), ordenado por pickup\_datetime
    
*   Puntos espaciados a lo largo de cada mes
    

**Ejemplo de uso:** years=\[2022,2023,2024\], rows\_per\_year=50000 ‚áí 150k filas (50k por a√±o, balanceadas 12√ó)

EDA (An√°lisis Exploratorio de Datos)
------------------------------------

*   **Nulos:** No cr√≠ticos en las columnas seleccionadas tras filtros
    
*   **Target (total\_amount):** Sesgo a valores bajos con outliers positivos (aeropuerto/largos viajes)
    
*   **Cardinalidad:** pu\_zone Top-20 + "Other" (limita cardinalidad en c√≥digo)
    
*   **Variables categ√≥ricas:** vendor\_id, rate\_code\_id, service\_type, pu\_borough con baja-media cardinalidad ‚Üí OHE
    
*   **Relaciones:** trip\_distance muy asociada a total\_amount; patrones horarios/d√≠a-semana visibles
    

Features Utilizadas
-------------------

### Num√©ricas

*   trip\_distance
    
*   passenger\_count
    
*   pickup\_hour
    
*   base\_fare\_components
    
*   congestion\_surcharge
    
*   airport\_fee
    
*   pickup\_dow
    
*   month
    
*   year
    

### Categ√≥ricas

*   service\_type
    
*   vendor\_id
    
*   rate\_code\_id
    
*   pu\_borough
    
*   pu\_zone\_processed (Top-20 + "Other")
    

### Binarias

*   is\_rush\_hour
    
*   is\_weekend
    
*   is\_night
    

(Todas creadas antes del split para evitar KeyErrors y garantizar paridad)

Split Temporal
--------------

*   **Entrenamiento:** 2022
    
*   **Validaci√≥n:** 2023
    
*   **Test:** 2024
    
*   **Caracter√≠sticas:** Expl√≠cito por a√±os, determin√≠stico (orden por pickup\_datetime), equidad de tama√±o: 50k registros por a√±o (si se usa rows\_per\_year=50000)
    
*   **Ventaja:** Evita aprender del futuro y garantiza reproducibilidad
    

Preprocesamiento Aplicado
-------------------------
| Tipo Variable | Transformaci√≥n | Par√°metros |
|---------------|----------------|------------|
| Num√©ricas | Imputaci√≥n | `SimpleImputer(strategy='median')` |
| Num√©ricas | Escalado | `StandardScaler()` |
| Categ√≥ricas | Imputaci√≥n | `SimpleImputer(fill_value='missing')` |
| Categ√≥ricas | Codificaci√≥n | `OneHotEncoder(handle_unknown='ignore', sparse_output=False)` |
| Binarias | Imputaci√≥n + Transformaci√≥n | `SimpleImputer(strategy='most_frequent')` + `FunctionTransformer` a float |
| Polinomios | Features Polin√≥micos | `PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)`<br>*Solo en: `trip_distance` y `base_fare_components` ‚Üí 3 columnas adicionales* |

El mismo ColumnTransformer y PolynomialFeatures se usan en todos los modelos ‚Üí garantiza paridad total entre versiones from-scratch y scikit-learn.

Modelos From Scratch (NumPy)
----------------------------

Implementaciones propias con la misma matriz de entrada:

### SGD (MSE + L2 opcional)

*   **Ventaja:** Simple, reproducible y estable con escalado; alpha mitiga sobreajuste
    
*   **HP expuestos:** learning\_rate (Œ∑), max\_iter, alpha (L2), tol
    

### Ridge (L2, soluci√≥n cerrada)
    
*   **Implementaci√≥n:** np.linalg.solve, m√°s estable que invertir
    
*   **Robustez:** Se a√±ade eps al diagonal
    
*   **HP:** alpha
    

### Lasso (L1, Coordinate Descent)

*   **Actualizaci√≥n:** Soft-thresholding por coordenada
*   **Efecto:** Esparsidad (coeficientes exactos en 0)
*   **HP:** alpha, max\_iter, tol
    

### Elastic Net (L1+L2, Coordinate Descent)

    
*   **HP:** alpha, l1\_ratio, max\_iter, tol
    

**Tuning (from-scratch):** Rejillas pragm√°ticas y peque√±as sobre alpha, l1\_ratio, learning\_rate, max\_iter; registro de tiempos y mejor RMSE en validaci√≥n. Esparsidad: se reporta n\_coefficients != 0.

Modelos Scikit-learn
--------------------

Equivalentes con misma X\_\*\_final, split y seed:

*   SGDRegressor(loss="squared\_error", penalty="l2", alpha, max\_iter\[, learning\_rate, eta0\])
    
*   Ridge(alpha)
    
*   Lasso(alpha)
    
*   ElasticNet(alpha, l1\_ratio)
    

**Tuning (sklearn):** GridSearchCV con rejillas peque√±as y comparables (actualmente alpha/max\_iter; recomendable a√±adir learning\_rate/eta0 en SGDRegressor para paridad completa con from-scratch).

Baselines
---------

Media y Mediana del train como predictores constantes (comparaci√≥n de piso).

---

 Resultados cuantitativos
-------------------------------

### Validaci√≥n 
| Modelo | RMSE | MAE | R¬≤ | Tiempo (s) |
|--------|------|-----|----|------------|
|  Ridge (From Scratch) | 3.296 | 2.131 | 0.9754 | 0.02 |
|  Ridge (Sklearn) | 3.300 | 2.135 | 0.9754 | 1.78 |
|  Lasso (From Scratch) | 3.302 | 2.097 | 0.9754 | 168.52 |
|  Lasso (Sklearn) | 3.309 | 2.121 | 0.9752 | 72.02 |
|  ElasticNet (From Scratch) | 3.344 | 2.103 | 0.9747 | 368.99 |
|  ElasticNet (Sklearn) | 3.328 | 2.127 | 0.9750 | 113.80 |
|  SGD (From Scratch) | 3.489 | 2.141 | 0.9725 | 15.23 |
|  SGD (Sklearn) | 3.299 | 2.105 | 0.9754 | 9.14 |
|  Baseline (Mean) | 22.296 | 12.551 | -0.124 | 0.00 |
|  Baseline (Median) | 24.474 | 13.816 | -0.354 | 0.00 |

###  Test 

| Modelo | RMSE | MAE | R¬≤ |
|--------|------|-----|----|
| Ridge (Sklearn) | 3.270 | 1.998 | 0.9759 |
|  Ridge (From Scratch) | 3.270 | 1.998 | 0.9759 |
|  Lasso (Sklearn) | 3.286 | 2.031 | 0.9757 |
| Lasso (From Scratch) | 3.290 | 2.034 | 0.9756 |
|  ElasticNet (Sklearn) | 3.301 | 2.027 | 0.9755 |
|  ElasticNet (From Scratch) | 3.325 | 2.029 | 0.9751 |
|  SGD (Sklearn) | 3.301 | 2.002 | 0.9755 |
|  SGD (From Scratch) | 3.450 | 2.040 | 0.9732 |
|  Baseline (Mean) | 22.332 | 12.510 | -0.122 |
|  Baseline (Median) | 24.501 | 13.770 | -0.351 |

 Diagn√≥stico y an√°lisis cualitativo
-----------------------------------------

###  An√°lisis del RMSE y R¬≤

*   Los **mejores modelos (Ridge FS y Ridge SKL)** presentan un **RMSE ‚âà $3.27**, lo que implica un error promedio cuadr√°tico muy bajo comparado con tarifas promedio entre $20‚Äì30.‚Üí Error relativo ‚âà **10‚Äì12 %**, excelente para un modelo lineal con datos reales de transporte.
    
*   El **R¬≤ ‚âà 0.976** indica que el modelo explica el **97.6 % de la variabilidad** del total\_amount, demostrando una relaci√≥n fuertemente lineal con las features seleccionadas.
    

###  An√°lisis del MAE (Mean Absolute Error)

*   El **MAE ‚âà $2.00** significa que el modelo, en promedio, **falla por ¬±2 USD por viaje**.En t√©rminos operativos:
    
    *   En un viaje de $25, el error t√≠pico es del **8 %**.
        
    *   Para el 91 % de los viajes, el error es **menor a $5**, lo que representa un desempe√±o excelente para predicciones en tiempo real.
        
*   El MAE bajo tambi√©n sugiere **ausencia de sesgos sistem√°ticos** (no sobrepredice ni subpredice de forma constante).
    

###  Comportamiento de los residuos

*   Los **residuos** se distribuyen de forma **sim√©trica y centrada en 0** ‚Üí sin sesgo evidente.
    
*   Errores mayores se concentran en **viajes largos o tarifas con recargos especiales**, donde los modelos lineales tienden a **subestimar**.
    
*   Entre $5 y $80 (rango operativo normal), la predicci√≥n es **muy precisa**.
    

 Conclusiones finales
---------------------------

 **Pipeline robusto, coherente y reproducible**Incluye limpieza, limitaci√≥n de cardinalidad, split temporal y preprocesamiento unificado.

 **Sin data leakage**Solo variables conocidas al inicio del viaje (pickup), sin usar dropoff ni informaci√≥n futura.

 **Modelos from-scratch funcionales y consistentes**Reproducen el comportamiento y m√©tricas de Scikit-learn con diferencias < 0.05 en RMSE.

 **Modelo ganador: Ridge (From Scratch)**

*   RMSE Validation: **$3.30**
    
*   RMSE Test: **$3.27**
    
*   MAE Test: **$2.00**
    
*   R¬≤ Test: **0.9759**
    
*   ŒîRMSE Test‚ÄìVal: **‚Äì0.03 (‚Äì0.8%)** ‚Üí sin sobreajuste.
    

### üìà Conclusi√≥n global

El pipeline cumple **todos los criterios del enunciado**:

*   Misma ingenier√≠a de features para sklearn y scratch.
    
*   Split temporal expl√≠cito y reproducible.
    
*   Regularizaci√≥n bien ajustada (Œ±, l1\_ratio).
    
*   M√©tricas claras y consistentes.
    

üöÄ Resultado:

> El modelo Ridge (From Scratch) logra rendimiento **de nivel producci√≥n**, con **RMSE ‚âà $3.27** y **MAE ‚âà $2.00**, explicando casi toda la variabilidad del precio final del viaje.


## 1Ô∏è‚É£5Ô∏è‚É£ Evidencias adjuntas

| Evidencia              | Archivo                          | Descripci√≥n                                      |
|------------------------|----------------------------------|--------------------------------------------------|
| üßæ **Capturas_Obt-builder**             | `obt_logs_1,obt_logs_2. obt_docker.jpg`                      | LOGS del proceso de obt.py     |
| üß© **Tablas comparativas**        | `validation y test.jpg`                 | M√©tricas RMSE / MAE / R¬≤ en validaci√≥n y test     |
| üî¢ **Errores por bucket**           | `errores_evidencia y errores_evidencia2.jpg`                    |         Gr√°fica y estadisticas de errores  |

---



**Checklist de aceptaci√≥n**
* RAW en Postgres: raw.yellow\_taxi\_trip, raw.green\_taxi\_trip, raw.taxi\_zone\_lookup (2015‚Äì2025).
* OBT analytics.obt\_trips creada por obt-builder (comando reproducible, logs).
* ML: 4 modelos from-scratch + 4 sklearn (mismo preprocesamiento y split).
* Comparativa: tabla RMSE/MAE/R¬≤ (validaci√≥n y test) + tiempos.
* Diagn√≥stico: residuales y errores por buckets. ‚óè README: comandos de ingesta, creaci√≥n OBT (comando que yo ejecutar√©), ejecuci√≥n notebook, variables .env. ‚óè Seeds fijas; resultados reproducibles.

---
